{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "article_recommend_training_extreme_multiple_classification.ipynb",
      "version": "0.3.2",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "[View in Colaboratory](https://colab.research.google.com/github/muik/notebooks/blob/master/experiments/article_recommend_training_extreme_multiple_classification.ipynb)"
      ]
    },
    {
      "metadata": {
        "id": "2cabk4sxIhbk",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "16a5c0d4-4232-4375-b1c0-f98c6078f873"
      },
      "cell_type": "code",
      "source": [
        "from google.colab import auth\n",
        "auth.authenticate_user()\n",
        "\n",
        "# https://cloud.google.com/resource-manager/docs/creating-managing-projects\n",
        "project_id = 'towneers'\n",
        "!gcloud config set project {project_id}"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Updated property [core/project].\r\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "CDJe48wLInA2",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 168
        },
        "outputId": "74a3d844-a7cb-4bd2-de74-600d88f6ba03"
      },
      "cell_type": "code",
      "source": [
        "%%bash\n",
        "gsutil -m cp gs://daangn-tmp/ml/data/user_article_links/train*.gz  /tmp/\n",
        "gsutil -m cp gs://daangn-tmp/ml/data/user_article_links/article_ids.txt  /tmp/"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Copying gs://daangn-tmp/ml/data/user_article_links/train1.tfrecord.gz...\n",
            "Copying gs://daangn-tmp/ml/data/user_article_links/train0.tfrecord.gz...\n",
            "Copying gs://daangn-tmp/ml/data/user_article_links/train2.tfrecord.gz...\n",
            "Copying gs://daangn-tmp/ml/data/user_article_links/train3.tfrecord.gz...\n",
            "/ [0/4 files][    0.0 B/148.3 MiB]   0% Done                                    \r/ [0/4 files][    0.0 B/148.3 MiB]   0% Done                                    \r/ [0/4 files][    0.0 B/148.3 MiB]   0% Done                                    \r/ [0/4 files][    0.0 B/148.3 MiB]   0% Done                                    \r-\r- [0/4 files][  6.7 MiB/148.3 MiB]   4% Done                                    \r\\\r\\ [1/4 files][ 55.9 MiB/148.3 MiB]  37% Done                                    \r|\r/\r/ [2/4 files][104.8 MiB/148.3 MiB]  70% Done                                    \r/ [3/4 files][111.9 MiB/148.3 MiB]  75% Done                                    \r-\r- [3/4 files][114.2 MiB/148.3 MiB]  77% Done                                    \r\\\r\\ [4/4 files][148.3 MiB/148.3 MiB] 100% Done                                    \r\n",
            "Operation completed over 4 objects/148.3 MiB.                                    \n",
            "Copying gs://daangn-tmp/ml/data/user_article_links/article_ids.txt...\n",
            "/ [0/1 files][    0.0 B/  2.3 MiB]   0% Done                                    \r/ [0/1 files][264.0 KiB/  2.3 MiB]  11% Done                                    \r-\r- [1/1 files][  2.3 MiB/  2.3 MiB] 100% Done                                    \r\n",
            "Operation completed over 1 objects/2.3 MiB.                                      \n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "metadata": {
        "id": "unoZCUbpIoRQ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "5c09a544-2a77-4fbb-a411-9859fce5034f"
      },
      "cell_type": "code",
      "source": [
        "article_id_set = [int(x.rstrip()) for x in open('/tmp/article_ids.txt', 'r').readlines()]\n",
        "article_id_to_idx = dict([(id, i) for i, id in enumerate(article_id_set)])\n",
        "print('articles count:', len(article_id_set))"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "articles count: 297197\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "czTtOcMMIpZ5",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import multiprocessing\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow.python.lib.io import file_io\n",
        "\n",
        "def read_examples(input_files, batch_size, shuffle=True, num_epochs=None):\n",
        "  \"\"\"Creates readers and queues for reading example protos.\"\"\"\n",
        "  files = []\n",
        "  for e in input_files:\n",
        "    for path in e.split(','):\n",
        "      files.extend(file_io.get_matching_files(path))\n",
        "  thread_count = multiprocessing.cpu_count()\n",
        "  \n",
        "  # The minimum number of instances in a queue from which examples are drawn\n",
        "  # randomly. The larger this number, the more randomness at the expense of\n",
        "  # higher memory requirements.\n",
        "  min_after_dequeue = 1000\n",
        "  \n",
        "  # When batching data, the queue's capacity will be larger than the batch_size\n",
        "  # by some factor. The recommended formula is (num_threads + a small safety\n",
        "  # margin). For now, we use a single thread for reading, so this can be small.\n",
        "  queue_size_multiplier = thread_count + 3\n",
        "  \n",
        "  # Convert num_epochs == 0 -> num_epochs is None, if necessary\n",
        "  num_epochs = num_epochs or None\n",
        "  \n",
        "  # Build a queue of the filenames to be read.\n",
        "  filename_queue = tf.train.string_input_producer(files, num_epochs, shuffle)\n",
        "  \n",
        "  options = tf.python_io.TFRecordOptions(\n",
        "      compression_type=tf.python_io.TFRecordCompressionType.GZIP)\n",
        "  example_id, encoded_example = tf.TFRecordReader(options=options).read_up_to(\n",
        "      filename_queue, batch_size)\n",
        "  \n",
        "  if shuffle:\n",
        "    capacity = min_after_dequeue + queue_size_multiplier * batch_size\n",
        "    return tf.train.shuffle_batch(\n",
        "        [example_id, encoded_example],\n",
        "        batch_size,\n",
        "        capacity,\n",
        "        min_after_dequeue,\n",
        "        enqueue_many=True,\n",
        "        num_threads=thread_count)\n",
        "        \n",
        "  else:\n",
        "    capacity = queue_size_multiplier * batch_size\n",
        "    return tf.train.batch(\n",
        "        [example_id, encoded_example],\n",
        "        batch_size,\n",
        "        capacity=capacity,\n",
        "        enqueue_many=True,\n",
        "        num_threads=thread_count)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "S9YXWf8YPoKr",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def dot_mode(latent_codes_1, latent_codes_2):\n",
        "    return tf.reduce_sum(latent_codes_1 * latent_codes_2, axis=-1)\n",
        "\n",
        "def cos_mode(latent_codes_1, latent_codes_2):\n",
        "    sq_norm_1 = tf.reduce_sum(latent_codes_1 ** 2, axis=-1)\n",
        "    sq_norm_2 = tf.reduce_sum(latent_codes_2 ** 2, axis=-1)\n",
        "    dot = dot_mode(latent_codes_1, latent_codes_2)\n",
        "    return dot / tf.sqrt(sq_norm_1 * sq_norm_2)\n",
        "\n",
        "def l2_mode(latent_codes_1, latent_codes_2):\n",
        "  return tf.norm(latent_codes_1 - latent_codes_2, ord=2, axis=-1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "5r7svsADQPB7",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "classifier = tf.estimator.Estimator()\n",
        "classifier.train(input_fn=lambda: my_input_fn(FILE_TRAIN, True, 500))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "hr-RhBZRIzKO",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1663
        },
        "outputId": "cd57dfc4-0374-46f1-d269-b2dae2e07bb8"
      },
      "cell_type": "code",
      "source": [
        "from time import time\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "\n",
        "from tensorflow.contrib import layers\n",
        "from tensorflow.python.keras.layers import Embedding\n",
        "\n",
        "DICT_SIZE = len(article_id_set)\n",
        "EMB_SIZE = 32\n",
        "NEGATIVE_SAMPLE_SIZE = 16\n",
        "\n",
        "def build_model(examples, training=True):\n",
        "  feature_map = {\n",
        "    'article_ids': tf.VarLenFeature(dtype=tf.int64),\n",
        "    'article_scores': tf.VarLenFeature(dtype=tf.float32),\n",
        "    'age': tf.FixedLenFeature(shape=[], dtype=tf.int64),\n",
        "    'target_id': tf.FixedLenFeature(shape=[], dtype=tf.int64),\n",
        "    'target_score': tf.FixedLenFeature(shape=[], dtype=tf.float32),\n",
        "  }\n",
        "  parsed = tf.parse_example(examples, features=feature_map)\n",
        "  article_ids = tf.sparse_tensor_to_dense(parsed['article_ids'], default_value=0)\n",
        "  article_scores = tf.sparse_tensor_to_dense(parsed['article_scores'], default_value=0.)\n",
        "  age = parsed['age'] # example age\n",
        "  target_id = parsed['target_id']\n",
        "  target_score = parsed['target_score']\n",
        "  \n",
        "  batch_size_tensor = tf.shape(age)[0]\n",
        "  negative_ids = tf.random_uniform([batch_size_tensor, NEGATIVE_SAMPLE_SIZE],\n",
        "                                   maxval=DICT_SIZE, dtype=tf.int64)\n",
        "\n",
        "  initializer = tf.random_uniform_initializer(-1., 1.)\n",
        "  article_embedding = Embedding(DICT_SIZE, EMB_SIZE,\n",
        "                                embeddings_initializer=initializer)\n",
        "\n",
        "  pre_embeddings = article_embedding(article_ids)\n",
        "  pre_score_sums = tf.expand_dims(tf.reduce_sum(article_scores, 1), 1)\n",
        "  expanded_article_scores = tf.expand_dims(article_scores / pre_score_sums, -1)\n",
        "  pre_embedding = tf.reduce_sum(expanded_article_scores * pre_embeddings, 1)\n",
        "\n",
        "  expanded_age = tf.expand_dims(age, 1)\n",
        "  expanded_age = tf.to_float(expanded_age)\n",
        "  expanded_age = tf.layers.batch_normalization(expanded_age, training=training)\n",
        "  \n",
        "  user = tf.concat([pre_embedding, expanded_age], 1)\n",
        "  user = layers.fully_connected(user, EMB_SIZE, activation_fn=tf.nn.tanh)\n",
        "\n",
        "  target_ids = tf.concat([tf.expand_dims(target_id, 1), negative_ids], 1)\n",
        "  target_embeddings = article_embedding(target_ids)\n",
        "  # [1, 0, 0, 0] => [0]\n",
        "  \n",
        "  logits = tf.matmul(tf.expand_dims(user, 1), target_embeddings, transpose_b=True)\n",
        "  logits = tf.squeeze(logits, 1)\n",
        "  prediction = tf.argmax(logits, 1)\n",
        "  labels = tf.zeros([batch_size_tensor], dtype=tf.int32)\n",
        "\n",
        "  cross_entropy = tf.nn.sparse_softmax_cross_entropy_with_logits(\n",
        "      logits=logits, labels=labels, name='xentropy')\n",
        "  loss_op = tf.reduce_mean(cross_entropy, name='xentropy_mean')\n",
        "  \n",
        "  all_logits = tf.matmul(user,\n",
        "                         article_embedding.variables[0],\n",
        "                         transpose_b=True)\n",
        "  is_in_top_k = tf.nn.in_top_k(all_logits, target_id, 100)\n",
        "  accuracy = tf.reduce_mean(tf.to_float(is_in_top_k))\n",
        "\n",
        "  global_step = tf.Variable(0, name='global_step', trainable=False)\n",
        "  \n",
        "  update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\n",
        "  with tf.control_dependencies(update_ops):\n",
        "    train_op = tf.train.AdamOptimizer(0.001).minimize(loss_op, global_step)\n",
        "  \n",
        "  return train_op, {\n",
        "      'global_step': global_step, 'loss': loss_op, 'eval': accuracy\n",
        "  }, [logits, prediction, user]\n",
        "\n",
        "#!rm -rf /tmp/train\n",
        "with tf.Graph().as_default():\n",
        "  checkpoint_dir = '/tmp/train'\n",
        "  max_steps = 3000000\n",
        "  batch_size = 128\n",
        "  keys, examples = read_examples(['/tmp/train*.tfrecord.gz'], batch_size, num_epochs=10)\n",
        "\n",
        "  train_op, logging_tensors, debug_tensors = build_model(examples)\n",
        "  \n",
        "  logging_hook = tf.train.LoggingTensorHook(\n",
        "          tensors=logging_tensors,\n",
        "          every_n_iter=5000)\n",
        "  hooks=[logging_hook, tf.train.StopAtStepHook(last_step=max_steps)]\n",
        "\n",
        "  with tf.train.MonitoredTrainingSession(\n",
        "      checkpoint_dir=checkpoint_dir, hooks=hooks, log_step_count_steps=10000,\n",
        "      save_checkpoint_secs=300) as session:\n",
        "    i = 0\n",
        "    while not session.should_stop():\n",
        "      session.run(train_op)\n",
        "      \n",
        "      if False:\n",
        "        if i % 1000 == 0:\n",
        "          results = session.run(debug_tensors)\n",
        "          for i, x in enumerate(results):\n",
        "            print(i, x[:2])\n",
        "        i += 1\n",
        "\n",
        "#!gsutil -m cp -r /tmp/train gs://daangn-tmp/ml/data/user_article_links/train"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Create CheckpointSaverHook.\n",
            "INFO:tensorflow:Graph was finalized.\n",
            "INFO:tensorflow:Restoring parameters from /tmp/train/model.ckpt-1822556\n",
            "INFO:tensorflow:Running local_init_op.\n",
            "INFO:tensorflow:Done running local_init_op.\n",
            "INFO:tensorflow:Saving checkpoints for 1822557 into /tmp/train/model.ckpt.\n",
            "INFO:tensorflow:global_step = 1822557, loss = 0.008787343, eval = 0.4375\n",
            "INFO:tensorflow:global_step = 1827557, loss = 0.052560907, eval = 0.5625 (53.080 sec)\n",
            "INFO:tensorflow:global_step/sec: 94.3097\n",
            "INFO:tensorflow:global_step = 1832557, loss = 0.009392569, eval = 0.5703125 (52.739 sec)\n",
            "INFO:tensorflow:global_step = 1837557, loss = 0.03137827, eval = 0.484375 (52.626 sec)\n",
            "INFO:tensorflow:global_step/sec: 94.8469\n",
            "INFO:tensorflow:global_step = 1842557, loss = 0.030319199, eval = 0.4375 (52.802 sec)\n",
            "INFO:tensorflow:global_step = 1847557, loss = 0.003398628, eval = 0.5390625 (52.857 sec)\n",
            "INFO:tensorflow:Saving checkpoints for 1850939 into /tmp/train/model.ckpt.\n",
            "INFO:tensorflow:global_step/sec: 94.4136\n",
            "INFO:tensorflow:global_step = 1852557, loss = 0.010487973, eval = 0.546875 (53.066 sec)\n",
            "INFO:tensorflow:global_step = 1857557, loss = 0.007740522, eval = 0.5 (53.066 sec)\n",
            "INFO:tensorflow:global_step/sec: 94.2818\n",
            "INFO:tensorflow:global_step = 1862557, loss = 0.0050311675, eval = 0.5078125 (52.996 sec)\n",
            "INFO:tensorflow:global_step = 1867557, loss = 0.024947954, eval = 0.4765625 (53.179 sec)\n",
            "INFO:tensorflow:global_step/sec: 93.8539\n",
            "INFO:tensorflow:global_step = 1872557, loss = 0.04563867, eval = 0.578125 (53.371 sec)\n",
            "INFO:tensorflow:global_step = 1877557, loss = 0.015096322, eval = 0.484375 (52.970 sec)\n",
            "INFO:tensorflow:Saving checkpoints for 1879173 into /tmp/train/model.ckpt.\n",
            "INFO:tensorflow:global_step/sec: 94.3423\n",
            "INFO:tensorflow:global_step = 1882557, loss = 0.028581914, eval = 0.53125 (53.027 sec)\n",
            "INFO:tensorflow:global_step = 1887557, loss = 0.01815088, eval = 0.453125 (52.850 sec)\n",
            "INFO:tensorflow:global_step/sec: 94.7363\n",
            "INFO:tensorflow:global_step = 1892557, loss = 0.031885378, eval = 0.40625 (52.705 sec)\n",
            "INFO:tensorflow:global_step = 1897557, loss = 0.026891299, eval = 0.5390625 (52.662 sec)\n",
            "INFO:tensorflow:global_step/sec: 94.8249\n",
            "INFO:tensorflow:global_step = 1902557, loss = 0.010915062, eval = 0.5546875 (52.799 sec)\n",
            "INFO:tensorflow:global_step = 1907557, loss = 0.008857506, eval = 0.5546875 (52.728 sec)\n",
            "INFO:tensorflow:Saving checkpoints for 1907583 into /tmp/train/model.ckpt.\n",
            "INFO:tensorflow:global_step/sec: 94.6283\n",
            "INFO:tensorflow:global_step = 1912557, loss = 0.021542853, eval = 0.515625 (52.942 sec)\n",
            "INFO:tensorflow:global_step = 1917557, loss = 0.05569861, eval = 0.453125 (52.842 sec)\n",
            "INFO:tensorflow:global_step/sec: 94.5472\n",
            "INFO:tensorflow:global_step = 1922557, loss = 0.025170635, eval = 0.46875 (52.928 sec)\n",
            "INFO:tensorflow:global_step = 1927557, loss = 0.010116736, eval = 0.53125 (52.764 sec)\n",
            "INFO:tensorflow:global_step/sec: 94.5893\n",
            "INFO:tensorflow:global_step = 1932557, loss = 0.014630226, eval = 0.5078125 (52.952 sec)\n",
            "INFO:tensorflow:Saving checkpoints for 1935957 into /tmp/train/model.ckpt.\n",
            "INFO:tensorflow:global_step = 1937557, loss = 0.0061717406, eval = 0.609375 (53.219 sec)\n",
            "INFO:tensorflow:global_step/sec: 94.3303\n",
            "INFO:tensorflow:global_step = 1942557, loss = 0.08459773, eval = 0.4375 (52.797 sec)\n",
            "INFO:tensorflow:global_step = 1947557, loss = 0.014346688, eval = 0.5859375 (53.094 sec)\n",
            "INFO:tensorflow:global_step/sec: 94.1609\n",
            "INFO:tensorflow:global_step = 1952557, loss = 0.001981166, eval = 0.6484375 (53.108 sec)\n",
            "INFO:tensorflow:global_step = 1957557, loss = 0.04142438, eval = 0.5625 (53.165 sec)\n",
            "INFO:tensorflow:global_step/sec: 94.0574\n",
            "INFO:tensorflow:global_step = 1962557, loss = 0.032590933, eval = 0.4453125 (53.154 sec)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Saving checkpoints for 1964190 into /tmp/train/model.ckpt.\n",
            "INFO:tensorflow:global_step = 1967557, loss = 0.054718792, eval = 0.53125 (53.390 sec)\n",
            "INFO:tensorflow:global_step/sec: 93.5659\n",
            "INFO:tensorflow:global_step = 1972557, loss = 0.013845446, eval = 0.4921875 (53.483 sec)\n",
            "INFO:tensorflow:global_step = 1977557, loss = 0.00697367, eval = 0.5234375 (53.287 sec)\n",
            "INFO:tensorflow:global_step/sec: 93.7477\n",
            "INFO:tensorflow:global_step = 1982557, loss = 0.018910915, eval = 0.5 (53.384 sec)\n",
            "INFO:tensorflow:global_step = 1987557, loss = 0.019925067, eval = 0.546875 (53.329 sec)\n",
            "INFO:tensorflow:Saving checkpoints for 1992306 into /tmp/train/model.ckpt.\n",
            "INFO:tensorflow:global_step/sec: 93.6378\n",
            "INFO:tensorflow:global_step = 1992557, loss = 0.015852392, eval = 0.4609375 (53.464 sec)\n",
            "INFO:tensorflow:global_step = 1997557, loss = 0.007155015, eval = 0.53125 (53.469 sec)\n",
            "INFO:tensorflow:global_step/sec: 93.5984\n",
            "INFO:tensorflow:global_step = 2002557, loss = 0.077112734, eval = 0.5390625 (53.368 sec)\n",
            "INFO:tensorflow:global_step = 2007557, loss = 0.024542496, eval = 0.5859375 (53.291 sec)\n",
            "INFO:tensorflow:global_step/sec: 93.6012\n",
            "INFO:tensorflow:global_step = 2012557, loss = 0.0020369487, eval = 0.546875 (53.549 sec)\n",
            "INFO:tensorflow:global_step = 2017557, loss = 0.0051433053, eval = 0.5859375 (53.529 sec)\n",
            "INFO:tensorflow:Saving checkpoints for 2020350 into /tmp/train/model.ckpt.\n",
            "INFO:tensorflow:global_step/sec: 93.2617\n",
            "INFO:tensorflow:global_step = 2022557, loss = 0.003985467, eval = 0.5859375 (53.695 sec)\n",
            "INFO:tensorflow:global_step = 2027557, loss = 0.10537253, eval = 0.5625 (53.411 sec)\n",
            "INFO:tensorflow:global_step/sec: 93.548\n",
            "INFO:tensorflow:global_step = 2032557, loss = 0.0035215295, eval = 0.5078125 (53.481 sec)\n",
            "INFO:tensorflow:global_step = 2037557, loss = 0.015986208, eval = 0.546875 (53.369 sec)\n",
            "INFO:tensorflow:global_step/sec: 93.7954\n",
            "INFO:tensorflow:global_step = 2042557, loss = 0.02223548, eval = 0.5625 (53.246 sec)\n",
            "INFO:tensorflow:global_step = 2047557, loss = 0.0072999587, eval = 0.5625 (53.446 sec)\n",
            "INFO:tensorflow:Saving checkpoints for 2048407 into /tmp/train/model.ckpt.\n",
            "INFO:tensorflow:global_step/sec: 93.2374\n",
            "INFO:tensorflow:global_step = 2052557, loss = 0.03395757, eval = 0.5390625 (53.812 sec)\n",
            "INFO:tensorflow:global_step = 2057557, loss = 0.05771441, eval = 0.4765625 (53.559 sec)\n",
            "INFO:tensorflow:global_step/sec: 92.8593\n",
            "INFO:tensorflow:global_step = 2062557, loss = 0.0133006135, eval = 0.59375 (54.125 sec)\n",
            "INFO:tensorflow:global_step = 2067557, loss = 0.0030348967, eval = 0.5625 (53.776 sec)\n",
            "INFO:tensorflow:global_step/sec: 93.0153\n",
            "INFO:tensorflow:global_step = 2072557, loss = 0.06770967, eval = 0.4453125 (53.733 sec)\n",
            "INFO:tensorflow:Saving checkpoints for 2076312 into /tmp/train/model.ckpt.\n",
            "INFO:tensorflow:global_step = 2077557, loss = 0.017673114, eval = 0.5 (53.751 sec)\n",
            "INFO:tensorflow:global_step/sec: 93.1949\n",
            "INFO:tensorflow:global_step = 2082557, loss = 0.014403317, eval = 0.390625 (53.556 sec)\n",
            "INFO:tensorflow:global_step = 2087557, loss = 0.009655915, eval = 0.59375 (53.672 sec)\n",
            "INFO:tensorflow:global_step/sec: 93.2054\n",
            "INFO:tensorflow:global_step = 2092557, loss = 0.028126607, eval = 0.4765625 (53.613 sec)\n",
            "INFO:tensorflow:Saving checkpoints for 2096741 into /tmp/train/model.ckpt.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "wzmjcCbMRtFt",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 336
        },
        "outputId": "266e655d-4fb0-4612-9bc7-a4914afbada0"
      },
      "cell_type": "code",
      "source": [
        "!gsutil -m cp -r /tmp/train gs://daangn-tmp/ml/data/user_article_links/train"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Copying file:///tmp/train/model.ckpt-1747834.index [Content-Type=application/octet-stream]...\r\n",
            "Copying file:///tmp/train/model.ckpt-1804325.meta [Content-Type=application/octet-stream]...\n",
            "Copying file:///tmp/train/model.ckpt-1776135.index [Content-Type=application/octet-stream]...\n",
            "Copying file:///tmp/train/model.ckpt-1804325.data-00000-of-00001 [Content-Type=application/octet-stream]...\n",
            "Copying file:///tmp/train/model.ckpt-1719444.meta [Content-Type=application/octet-stream]...\n",
            "Copying file:///tmp/train/model.ckpt-1747834.meta [Content-Type=application/octet-stream]...\n",
            "Copying file:///tmp/train/model.ckpt-1804325.index [Content-Type=application/octet-stream]...\n",
            "Copying file:///tmp/train/checkpoint [Content-Type=application/octet-stream]...\n",
            "Copying file:///tmp/train/model.ckpt-1822556.meta [Content-Type=application/octet-stream]...\n",
            "Copying file:///tmp/train/model.ckpt-1747834.data-00000-of-00001 [Content-Type=application/octet-stream]...\n",
            "Copying file:///tmp/train/model.ckpt-1822556.data-00000-of-00001 [Content-Type=application/octet-stream]...\n",
            "Copying file:///tmp/train/graph.pbtxt [Content-Type=application/octet-stream]...\n",
            "Copying file:///tmp/train/model.ckpt-1776135.meta [Content-Type=application/octet-stream]...\n",
            "Copying file:///tmp/train/model.ckpt-1719444.index [Content-Type=application/octet-stream]...\n",
            "Copying file:///tmp/train/model.ckpt-1719444.data-00000-of-00001 [Content-Type=application/octet-stream]...\n",
            "Copying file:///tmp/train/model.ckpt-1822556.index [Content-Type=application/octet-stream]...\n",
            "Copying file:///tmp/train/model.ckpt-1776135.data-00000-of-00001 [Content-Type=application/octet-stream]...\n",
            "\\\n",
            "Operation completed over 17 objects/545.2 MiB.                                   \n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}